{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions\n",
    "import sys,os\n",
    "import glob\n",
    "try: \n",
    "    if(cwd is not None):\n",
    "        from functions.utils_curation import *\n",
    "except:\n",
    "    %cd ..\n",
    "    cwd = os.getcwd()\n",
    "    sys.path.insert(0,cwd)\n",
    "    from functions.utils_curation import *\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation and standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "df0 = check_extention('single-protein-assay-ic50-only.csv', 1)\n",
    "\n",
    "#save summary\n",
    "row = ['inital', len(df0)]\n",
    "with open('./data/data_summary/preparation_summary.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing activity values\n",
    "df0 = df0.dropna(subset=['Standard Value'])\n",
    "\n",
    "#save summary\n",
    "row = ['after missing activity removed', len(df0)]\n",
    "with open('./data/data_summary/preparation_summary.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "df0 = df0.reset_index(drop=True)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if every activity unit is the same\n",
    "df0.groupby('Standard Units').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted activity data\n",
    "diffact = []\n",
    "DropList = []\n",
    "for index, activity in enumerate(df0['Standard Units']):\n",
    "    if str(activity).lower() == 'nm':\n",
    "        pass\n",
    "    else:\n",
    "        diffact.append(df0.iloc[[index]])\n",
    "        DropList.append(index)\n",
    "\n",
    "if len(diffact) == 0:\n",
    "    row = [\"after different activity removed\", len(df0)]\n",
    "    with open('./data/data_summary/preparation_summary.csv','a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)\n",
    "else:\n",
    "    df0 = df0.drop(DropList, errors=\"ignore\")\n",
    "    diffact = pd.concat(diffact)\n",
    "    diffact.to_csv('./data/removed_during_curation/different_activity.csv')\n",
    "    df0 = df0.reset_index()\n",
    "    row = [\"after different activity removed\", len(df0)]\n",
    "    with open('./data/data_summary/preparation_summary.csv','a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted columns\n",
    "dropList = ['Standard Units']\n",
    "df0 = df0.drop(columns = dropList)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Optional step, make sure to adapt the code to your dataset \"\"\"\n",
    "#convert IC50 to pIC50\n",
    "pIC50 = []\n",
    "for value in df0['Standard Value']:\n",
    "    value = value / 1000\n",
    "    pIC50.append(-(math.log10(value*10**-6)))\n",
    "\n",
    "df0['pIC50 (uM)'] = pIC50\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns if needed\n",
    "df0.rename(columns = {'Molecule ChEMBL ID':'ID', 'Smiles':'SMILES', 'Standard Relation':'Relation', 'Standard Value': 'IC50 (uM)'}, inplace = True)\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structural curation\n",
    "curated_dataset = curate(df0, save_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate removal binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df1 = check_extention('standardised_but_no_duplicates_removed.csv', 2)\n",
    "\n",
    "# summary\n",
    "row = [\"initial\", len(df1)]\n",
    "with open('./data/data_summary/binary_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check types of relation\n",
    "df1.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = relationTreat(dataset = df1, relationcolumn = 'Relation', activitycolumn = 'IC50 (uM)', threshold = 10, curationtype = 'binary')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define threshold\n",
    "outcome = [1 if activity > 5 else 0 for activity in df1['pIC50 (uM)']]\n",
    "df1['Outcome'] = outcome\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df1_agg = group(df1, ['pIC50 (uM)', 'ID', 'SMILES', 'Outcome'])\n",
    "\n",
    "# summary\n",
    "row = [\"duplicates total\", len(df1) - len(df1_agg)]\n",
    "with open('./data/data_summary/binary_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "df1_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates with stddev > 0\n",
    "df2_agg = dupRemovalClassification(df1_agg, 'Outcome', 'binary')\n",
    "\n",
    "# summary\n",
    "row = [\"discordant duplicates\", (len(df1_agg) - len(df2_agg))]\n",
    "with open('./data/data_summary/binary_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "df2_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = removeListedValues(df2_agg)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df1.to_csv(f'./data/curated_data/curated_binary.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate removal regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df2 = check_extention(fname, 2)\n",
    "\n",
    "# summary\n",
    "row = [\"initial\", len(df2)]\n",
    "with open('./data/data_summary/regression_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove relations != \"=\"\n",
    "\n",
    "index_drop = []\n",
    "for index, relation in enumerate(df2['Relation']):\n",
    "    if relation != \"'='\":\n",
    "        index_drop.append(index)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if len(index_drop) == 0:\n",
    "    pass\n",
    "else:\n",
    "    mask = df2.iloc[index_drop]\n",
    "    df2_removed = df2.drop(index_drop, errors=\"ignore\").reset_index(drop = True)\n",
    "    mask.to_csv(\"{}relationsRemoved_regression.csv\".format(errorverbose), sep=',', header=True, index=False)\n",
    "\n",
    "# summary\n",
    "row = [\"removed relations != '='\", len(df2) - len(df2_removed)]\n",
    "with open('./data/data_summary/regression_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "df2_removed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check relations\n",
    "df2_removed.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop relations column\n",
    "df2 = df2_removed.drop(columns = 'Relation')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df2_agg = group(df2, ['pIC50 (uM)', 'ID', 'SMILES'])\n",
    "\n",
    "# summary\n",
    "row = [\"duplicates total\", len(df2) - len(df2_agg)]\n",
    "with open('./data/data_summary/regression_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_agg = dupRemovalRegression(df2_agg, errorverbose, 'pIC50 (uM)', 0.2)\n",
    "\n",
    "# summary\n",
    "row = [\"discordant duplicates\", len(df2_agg) - len(df3_agg)]\n",
    "with open('./data/data_summary/regression_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df3_agg   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = removeListedValues(df3_agg)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df2.to_csv(f'{save_data}curated_regression.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate removal multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df3 = check_extention(fname, 2)\n",
    "# summary\n",
    "row = [\"initial\", len(df3)]\n",
    "with open('./data/data_summary/multiclass_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check types of relation\n",
    "df3.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = relationTreat(dataset = df3, relationcolumn = 'Relation', activitycolumn = 'IC50 (uM)', threshold = 10, curationtype = 'multiclass')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define threshold\n",
    "outcome = []\n",
    "\n",
    "for activity in df3['pIC50 (uM)']:\n",
    "    #nonblocker\n",
    "    if activity < 4.5:\n",
    "        outcome.append(0)\n",
    "    elif activity < 5 and activity >= 4.5:\n",
    "        outcome.append(1)\n",
    "    elif activity >= 5 and activity < 6:\n",
    "        outcome.append(2)\n",
    "    else:\n",
    "        outcome.append(3)\n",
    "df3['Outcome'] = outcome\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df3_agg = group(df3, ['pIC50 (uM)', 'ID', 'SMILES', 'Outcome'])\n",
    "\n",
    "# summary\n",
    "row = [\"duplicates total\", len(df3) - len(df3_agg)]\n",
    "with open('./data/data_summary/multiclass_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df3_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates with stddev > 0\n",
    "df4_agg = dupRemovalClassification(df3_agg, 'Outcome', 'multiclass')\n",
    "\n",
    "# summary\n",
    "row = [\"discordant duplicates\", len(df3_agg) - len(df4_agg)]\n",
    "with open('./data/data_summary/multiclass_dupremoval.csv','a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(row)\n",
    "\n",
    "df4_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = removeListedValues(df4_agg)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df3.to_csv(f'{save_data}curated_multiclass.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('chem_pipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a59e22669eba1dc6e4d930302cbb600e9d71d81892f6be3c25d1b723809ef057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
